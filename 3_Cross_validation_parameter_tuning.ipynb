{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3_Cross-validation_parameter-tuning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mariumabid/AI-challenge-/blob/master/3_Cross_validation_parameter_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76vFmUa3pDZp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn import preprocessing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHybXSht1XJC",
        "colab_type": "text"
      },
      "source": [
        "### Data pre-processing\n",
        "\n",
        "Only alter the data pre-processing code if you have completed the challenge for that section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMaxPvA4n9W_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The data URI\n",
        "csv_file_uri = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
        "\n",
        "column_names = [\n",
        "    \"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\", \"marital-status\",\n",
        "    \"occupation\", \"relationship\", \"race\", \"sex\", \"capital-gain\", \"capital-loss\",\n",
        "    \"hours-per-week\", \"native-country\", \"target\"\n",
        "]\n",
        "\n",
        "\n",
        "data_original = pd.read_csv(csv_file_uri, names=column_names, index_col=False)\n",
        "\n",
        "USE_LABEL_ENCODER = False\n",
        "\n",
        "\n",
        "if USE_LABEL_ENCODER:\n",
        "\n",
        "    # Make a copy so that we always have the original data to refer to\n",
        "    data = data_original.copy(deep=True)\n",
        "\n",
        "    # Drop the US weights (don't have any value)\n",
        "    data.drop([\"fnlwgt\"], axis=1, inplace=True)\n",
        "\n",
        "    # Create a function that changes the text to a simple binary value\n",
        "    def convert_target_variable(text):\n",
        "        if text == \" <=50K\":\n",
        "            return 0\n",
        "        else:\n",
        "            return 1\n",
        "\n",
        "    data[\"target_encoded\"] = data.target.apply(convert_target_variable)\n",
        "\n",
        "    # Deletes the original column in this dataframe.\n",
        "    data.drop([\"target\"], axis=1, inplace=True)\n",
        "\n",
        "    encoded_columns = []\n",
        "    for c in data.columns:\n",
        "        if data[c].dtype == \"object\":\n",
        "            if \"{}_encoded\".format(c) not in data.columns:\n",
        "                encoder = preprocessing.LabelEncoder()\n",
        "                data[\"{}_encoded\".format(c)] = encoder.fit_transform(data[c].values)\n",
        "                encoded_columns.append(c)\n",
        "                encoder = None\n",
        "            else:\n",
        "                print(\"{}_encoded already exists\".format(c))\n",
        "\n",
        "    print(\"Dropping the encoded columns {}\".format(encoded_columns))\n",
        "    data.drop(encoded_columns, axis=1, inplace=True)\n",
        "    \n",
        "else:\n",
        "    \n",
        "    # Make a copy so that we always have the original data to refer to\n",
        "    data_pre_dummies = data_original.copy(deep=True)\n",
        "\n",
        "    # Drop the US weights (don't have any value)\n",
        "    data_pre_dummies.drop([\"fnlwgt\"], axis=1, inplace=True)\n",
        "    \n",
        "    data = pd.get_dummies(data_pre_dummies)\n",
        "\n",
        "    # Deletes the original column in this dataframe.\n",
        "    data.drop([\"target_ <=50K\"], axis=1, inplace=True)\n",
        "\n",
        "    # Rename the target\n",
        "    data.rename(columns={'target_ >50K': 'target' }, inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwrNciDG1XJG",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "### Model\n",
        "\n",
        "This is the moment we can use the new, numerical, data to plug it into pretty much any classification model. First we'll convert the data to a matrix with our features - that is the data that we want to use to predict from - and an array with our labels - the target variable that indicates if someone makes more than 50k or not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8qcZVFMybnT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_columns = data.columns.tolist()\n",
        "feature_columns.remove(\"target\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PKvkNxwMyPN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = data[feature_columns].values\n",
        "y = data[\"target\"].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlojcJcw1XJM",
        "colab_type": "text"
      },
      "source": [
        "Lets use a model from **scikit-learn**: LogisticRegression\n",
        "\n",
        "For those interested in the [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XhqqOU0wfJv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import linear_model\n",
        "\n",
        "# Create linear regression object\n",
        "clf = linear_model.LogisticRegression()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7xfzkHY1XJQ",
        "colab_type": "code",
        "outputId": "9fb1ce1a-1aa3-4de9-c1cc-38d73f8df8eb",
        "colab": {}
      },
      "source": [
        "# MAGIC\n",
        "# \n",
        "# Train the model using the training sets\n",
        "clf.fit(X, y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
              "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
              "          verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_CskNhyw0Wl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make predictions using the testing set. \n",
        "# For now we'll use the last value of the training set.\n",
        "pred = clf.predict(X[-1,:].reshape(1,-1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t34fYx1i1XJX",
        "colab_type": "code",
        "outputId": "f60205ba-f8cc-45fd-863b-6133ec1e8391",
        "colab": {}
      },
      "source": [
        "# Print the data \n",
        "print(data_original.iloc[data_original.index[-1]])\n",
        "\n",
        "# and the prediction\n",
        "print(\"Predict its a {}\".format(pred))\n",
        "\n",
        "probability = clf.predict_proba(X[-1,:].reshape(1,-1))\n",
        "print(\"With a probability of {}\".format(probability[0,pred[0]]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "age                                52\n",
            "workclass                Self-emp-inc\n",
            "fnlwgt                         287927\n",
            "education                     HS-grad\n",
            "education-num                       9\n",
            "marital-status     Married-civ-spouse\n",
            "occupation            Exec-managerial\n",
            "relationship                     Wife\n",
            "race                            White\n",
            "sex                            Female\n",
            "capital-gain                    15024\n",
            "capital-loss                        0\n",
            "hours-per-week                     40\n",
            "native-country          United-States\n",
            "target                           >50K\n",
            "Name: 32560, dtype: object\n",
            "Predict its a [1]\n",
            "With a probability of 0.9941011699994576\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_M1dcBi1XJb",
        "colab_type": "code",
        "outputId": "76c57c11-589f-4240-9ed2-6b1247e912a8",
        "colab": {}
      },
      "source": [
        "# How did the model do this? We can see the coefficients \n",
        "# for each column using \n",
        "# print(clf.coef_)\n",
        "\n",
        "# Pretty print with the column names\n",
        "for ix, c in enumerate(feature_columns):\n",
        "    print(\"Column {} is {}\".format(c, clf.coef_[0][ix]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Column age is 0.023409486993560876\n",
            "Column education-num is 0.15129774371761853\n",
            "Column capital-gain is 0.0003111634986664115\n",
            "Column capital-loss is 0.0006361807980127145\n",
            "Column hours-per-week is 0.028087267936899286\n",
            "Column workclass_ ? is -0.569066267364764\n",
            "Column workclass_ Federal-gov is 0.21937537957450026\n",
            "Column workclass_ Local-gov is -0.4681449407137768\n",
            "Column workclass_ Never-worked is -0.006917804610970515\n",
            "Column workclass_ Private is -0.30611626467191855\n",
            "Column workclass_ Self-emp-inc is -0.1536813198588894\n",
            "Column workclass_ Self-emp-not-inc is -0.790084638461164\n",
            "Column workclass_ State-gov is -0.6082055740277911\n",
            "Column workclass_ Without-pay is -0.05276469396084467\n",
            "Column education_ 10th is -0.6651367064039237\n",
            "Column education_ 11th is -0.76340840669363\n",
            "Column education_ 12th is -0.3483914075684868\n",
            "Column education_ 1st-4th is -0.2135806616653651\n",
            "Column education_ 5th-6th is -0.3746372545952796\n",
            "Column education_ 7th-8th is -0.7675164823371127\n",
            "Column education_ 9th is -0.5499736838379005\n",
            "Column education_ Assoc-acdm is -0.25996935373845936\n",
            "Column education_ Assoc-voc is -0.11529667488082818\n",
            "Column education_ Bachelors is 0.1769644984193677\n",
            "Column education_ Doctorate is 0.6720998896586211\n",
            "Column education_ HS-grad is -0.33721722529942877\n",
            "Column education_ Masters is 0.36052781091513664\n",
            "Column education_ Preschool is -0.09988316982842362\n",
            "Column education_ Prof-school is 0.713439257373971\n",
            "Column education_ Some-college is -0.16362655361380232\n",
            "Column marital-status_ Divorced is -0.7056939941614938\n",
            "Column marital-status_ Married-AF-spouse is 0.0944219538310638\n",
            "Column marital-status_ Married-civ-spouse is 0.5632296514765764\n",
            "Column marital-status_ Married-spouse-absent is -0.3723405065644019\n",
            "Column marital-status_ Never-married is -1.258291110373705\n",
            "Column marital-status_ Separated is -0.6284964860315972\n",
            "Column marital-status_ Widowed is -0.42843563227207637\n",
            "Column occupation_ ? is -0.5759840719757341\n",
            "Column occupation_ Adm-clerical is -0.16455369175634302\n",
            "Column occupation_ Armed-Forces is -0.020243681722256497\n",
            "Column occupation_ Craft-repair is -0.101851429582781\n",
            "Column occupation_ Exec-managerial is 0.6061748964914272\n",
            "Column occupation_ Farming-fishing is -1.0359908296819385\n",
            "Column occupation_ Handlers-cleaners is -0.8270159903662561\n",
            "Column occupation_ Machine-op-inspct is -0.46637731739681576\n",
            "Column occupation_ Other-service is -1.0352875433616953\n",
            "Column occupation_ Priv-house-serv is -0.17107544648385248\n",
            "Column occupation_ Prof-specialty is 0.34078498492827564\n",
            "Column occupation_ Protective-serv is 0.37923359780903493\n",
            "Column occupation_ Sales is 0.08532729724336978\n",
            "Column occupation_ Tech-support is 0.5255194541589003\n",
            "Column occupation_ Transport-moving is -0.2742663523988851\n",
            "Column relationship_ Husband is -0.26317180091968445\n",
            "Column relationship_ Not-in-family is -0.5332567575906014\n",
            "Column relationship_ Other-relative is -0.7325597925341328\n",
            "Column relationship_ Own-child is -1.483237532477784\n",
            "Column relationship_ Unmarried is -0.7832224378796436\n",
            "Column relationship_ Wife is 1.0598421973061132\n",
            "Column race_ Amer-Indian-Eskimo is -0.5001493391206041\n",
            "Column race_ Asian-Pac-Islander is -0.6384462224401364\n",
            "Column race_ Black is -0.6737571988557188\n",
            "Column race_ Other is -0.44589269195647996\n",
            "Column race_ White is -0.47736067172260144\n",
            "Column sex_ Female is -1.8116563839634436\n",
            "Column sex_ Male is -0.9239497401323822\n",
            "Column native-country_ ? is -0.38051750004763446\n",
            "Column native-country_ Cambodia is 0.038911645497531795\n",
            "Column native-country_ Canada is 0.021177329258287043\n",
            "Column native-country_ China is -0.16489811782920002\n",
            "Column native-country_ Columbia is -0.16044101950940254\n",
            "Column native-country_ Cuba is -0.0022718120896120775\n",
            "Column native-country_ Dominican-Republic is -0.14552947283724055\n",
            "Column native-country_ Ecuador is -0.046562331314962076\n",
            "Column native-country_ El-Salvador is -0.1384937847905344\n",
            "Column native-country_ England is 0.016550672101190156\n",
            "Column native-country_ France is 0.033217988659539684\n",
            "Column native-country_ Germany is 0.034061563915125936\n",
            "Column native-country_ Greece is -0.09373263745787797\n",
            "Column native-country_ Guatemala is -0.06134617032192977\n",
            "Column native-country_ Haiti is -0.03789741235310579\n",
            "Column native-country_ Holand-Netherlands is -0.0017482966282406057\n",
            "Column native-country_ Honduras is -0.012380484007750148\n",
            "Column native-country_ Hong is -0.020796160751433906\n",
            "Column native-country_ Hungary is -0.01509890008052884\n",
            "Column native-country_ India is -0.15093190066556264\n",
            "Column native-country_ Iran is -0.03045389901426714\n",
            "Column native-country_ Ireland is 0.007732339206366155\n",
            "Column native-country_ Italy is 0.07871323763263657\n",
            "Column native-country_ Jamaica is -0.053107810124607004\n",
            "Column native-country_ Japan is 0.021214667548867767\n",
            "Column native-country_ Laos is -0.03349199226804642\n",
            "Column native-country_ Mexico is -0.7054261192727318\n",
            "Column native-country_ Nicaragua is -0.06768055255673809\n",
            "Column native-country_ Outlying-US(Guam-USVI-etc) is -0.03708115698035652\n",
            "Column native-country_ Peru is -0.04683382440026794\n",
            "Column native-country_ Philippines is 0.05733577360187878\n",
            "Column native-country_ Poland is -0.05945255796035233\n",
            "Column native-country_ Portugal is -0.05317311355508058\n",
            "Column native-country_ Puerto-Rico is -0.16468714392211584\n",
            "Column native-country_ Scotland is -0.004094945015944289\n",
            "Column native-country_ South is -0.18161974150483254\n",
            "Column native-country_ Taiwan is -0.02715235379376895\n",
            "Column native-country_ Thailand is -0.025172923842066038\n",
            "Column native-country_ Trinadad&Tobago is -0.02662256465790912\n",
            "Column native-country_ United-States is 0.03597579700883026\n",
            "Column native-country_ Vietnam is -0.14577079275760083\n",
            "Column native-country_ Yugoslavia is 0.013970353786019773\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNh6zR4VT71B",
        "colab_type": "code",
        "outputId": "39d5d737-6ddb-4f3a-97fd-5e6b7232be7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# How good is the model by evaluating it \n",
        "# on the training set\n",
        "print(\"This model has an overall accuracy of {}\".format(clf.score(X, y)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This model has an overall accuracy of 0.8511716470624366\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSqicifO1XJi",
        "colab_type": "text"
      },
      "source": [
        "**The above is incredibly bad practice (up to the point where I should be fired for even showing you). Why?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POUwYLbqUI4p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Exercise: Implement/Annotate Cross Validation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m30H5xcK1XJn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "np.random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5H_lyza1XJq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a training and a testing group.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxIDtKDH1XJs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To implement:\n",
        "# - Retrain the model.\n",
        "# - Test the model's performance on both the train and test data\n",
        "# - See how they change (by rerunning the cross-validation cells)\n",
        "#   when you change the test size.\n",
        "\n",
        "# Note, you are now working with matrices, not arrays (for prediction)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etjDIiUEUJOt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Challenge: Parameter tuning\n",
        "\n",
        "Logistic Regression: [docs](http://scikit-learn.org/stable/auto_examples/linear_model/plot_logistic_l1_l2_sparsity.html#sphx-glr-auto-examples-linear-model-plot-logistic-l1-l2-sparsity-py)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pf_mGe9u1XJ1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# There are three hyper-parameters in Logistic Regression you can change:\n",
        "# C, penalty and tol (tolerance)\n",
        "\n",
        "# See the docs above to change them.\n",
        "\n",
        "# The challenge is to test a number of different hyper parameters and find a set\n",
        "# that performs better than the default values"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}